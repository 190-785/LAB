{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 2**"
      ],
      "metadata": {
        "id": "_js5cfu-2VPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same\n",
        "using XOR Gate. Vary the activation functions and compare the results."
      ],
      "metadata": {
        "id": "Gl468E6e8A7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - x**2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def lrelu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def lrelu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def prelu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def prelu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "\n",
        "# Artificial Neural Network class\n",
        "class ANN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation):\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.output_size = output_size\n",
        "      self.activation = activation\n",
        "\n",
        "      self.weights1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "      #print(self.weights1)\n",
        "      self.bias1 = np.zeros((1, self.hidden_size))\n",
        "      self.weights2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "      #print(self.weights2)\n",
        "      self.bias2 = np.zeros((1, self.output_size))\n",
        "\n",
        "      # Activation function mapping (dictionary to easily choose from different functions)\n",
        "      self.activation_functions = {\n",
        "          'sigmoid': (sigmoid, sigmoid_derivative),\n",
        "          'tanh': (tanh, tanh_derivative),\n",
        "          'relu': (relu, relu_derivative),\n",
        "          'lrelu': (lrelu, lrelu_derivative),\n",
        "          'prelu': (prelu, prelu_derivative)\n",
        "      }\n",
        "\n",
        "      self.activation_function, self.activation_derivative = self.activation_functions[self.activation]\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer = self.activation_function(np.dot(X, self.weights1) + self.bias1)\n",
        "        self.output_layer = self.activation_function(np.dot(self.hidden_layer, self.weights2) + self.bias2)\n",
        "        return self.output_layer\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "      #Output Layer Error\n",
        "      error = y - self.output_layer\n",
        "      d_output = error * self.activation_derivative(self.output_layer)\n",
        "\n",
        "      #Error at Hidden Layer\n",
        "      error_hidden_layer = d_output.dot(self.weights2.T)\n",
        "      d_hidden_layer = error_hidden_layer * self.activation_derivative(self.hidden_layer) # Use the selected derivative function\n",
        "\n",
        "      #Update the weights and biases\n",
        "      self.weights2 += self.hidden_layer.T.dot(d_output) * learning_rate\n",
        "      self.bias2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
        "      self.weights1 += X.T.dot(d_hidden_layer) * learning_rate\n",
        "      self.bias1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for _ in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "#Sample data: XOR problem\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "               [1, 0],\n",
        "                [1, 1]])\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "               [1],\n",
        "                [0]])\n",
        "\n",
        "#Initialize and train network with sigmoid\n",
        "ann = ANN(2, 4, 1, activation='sigmoid')\n",
        "ann.train(X, y, 10000, 0.1)\n",
        "print(\"Sigmoid output:\")\n",
        "print(ann.forward(X))\n",
        "print(ann.forward(X).round())\n",
        "\n",
        "# Initialize and train network with tanh\n",
        "ann_tanh = ANN(2, 4, 1, activation='tanh')\n",
        "ann_tanh.train(X, y, 10000, 0.1)\n",
        "print(\"\\nTanh output:\")\n",
        "print(ann_tanh.forward(X))\n",
        "print(ann_tanh.forward(X).round())\n",
        "\n",
        "# Initialize and train network with tanh\n",
        "ann_tanh = ANN(2, 4, 1, activation='relu')\n",
        "ann_tanh.train(X, y, 10000, 0.1)\n",
        "print(\"\\nRelu output:\")\n",
        "print(ann_tanh.forward(X))\n",
        "print(ann_tanh.forward(X).round())\n",
        "\n",
        "# Initialize and train network with tanh\n",
        "ann_tanh = ANN(2, 4, 1, activation='lrelu')\n",
        "ann_tanh.train(X, y, 10000, 0.1)\n",
        "print(\"\\nLeaky Relu output:\")\n",
        "print(ann_tanh.forward(X))\n",
        "print(ann_tanh.forward(X).round())\n",
        "\n",
        "# Initialize and train network with tanh\n",
        "ann_tanh = ANN(2, 4, 1, activation='prelu')\n",
        "ann_tanh.train(X, y, 10000, 0.1)\n",
        "print(\"\\nParametric Relu output:\")\n",
        "print(ann_tanh.forward(X))\n",
        "print(ann_tanh.forward(X).round())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2gVyzkf5gAG",
        "outputId": "61502621-67c4-478a-bd76-fcbf8990b730"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.08709092  0.43518511  0.6179665   0.4907286 ]\n",
            " [-0.76700123  0.25516246  0.99691234  0.78542476]]\n",
            "[[ 0.4192113 ]\n",
            " [-0.43952005]\n",
            " [-2.65851276]\n",
            " [-0.23304615]]\n",
            "Sigmoid output:\n",
            "[[0.04914452]\n",
            " [0.95450391]\n",
            " [0.95203431]\n",
            " [0.04996888]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "[[ 0.55095101 -1.74909837 -1.46413557  0.82280817]\n",
            " [ 1.21248952 -2.23391004  2.52961201  0.13645453]]\n",
            "[[-0.13004653]\n",
            " [-0.21060411]\n",
            " [ 1.23475739]\n",
            " [ 0.3495158 ]]\n",
            "\n",
            "Tanh output:\n",
            "[[6.03447520e-05]\n",
            " [9.91262950e-01]\n",
            " [9.92019870e-01]\n",
            " [1.58601617e-04]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "[[ 0.10449699  0.50993372  2.24490812 -1.35693914]\n",
            " [-1.76909725 -0.12400751 -2.28942386  0.67659718]]\n",
            "[[ 1.24482937]\n",
            " [-0.95185553]\n",
            " [-0.87064609]\n",
            " [ 1.07140157]]\n",
            "\n",
            "Relu output:\n",
            "[[4.4408921e-16]\n",
            " [1.0000000e+00]\n",
            " [0.0000000e+00]\n",
            " [0.0000000e+00]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[ 1.10391985 -0.24832422  1.00219021 -1.07762639]\n",
            " [ 0.1566922  -0.46201533 -0.76768452 -0.38827396]]\n",
            "[[-0.23211275]\n",
            " [-1.80994619]\n",
            " [-0.50574662]\n",
            " [-0.92641114]]\n",
            "\n",
            "Leaky Relu output:\n",
            "[[8.8817842e-16]\n",
            " [1.0000000e+00]\n",
            " [1.0000000e+00]\n",
            " [4.4408921e-16]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n",
            "[[ 1.36085924 -1.43937449 -0.02105012 -0.73264446]\n",
            " [-0.47326469  1.85382595  1.03783607 -1.29552733]]\n",
            "[[-0.71833761]\n",
            " [ 0.52282558]\n",
            " [ 0.78842186]\n",
            " [ 1.11396624]]\n",
            "\n",
            "Parametric Relu output:\n",
            "[[-4.14355721e-05]\n",
            " [ 1.00000007e+00]\n",
            " [ 1.00000005e+00]\n",
            " [ 5.26585350e-08]]\n",
            "[[-0.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [ 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "KsqsBJpk5GUy"
      }
    }
  ]
}